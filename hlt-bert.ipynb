{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "b0390ea275c144ca9d62e949e137cfea",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 0,
    "execution_start": 1746591538432,
    "source_hash": "edbf9cb0"
   },
   "outputs": [],
   "source": [
    "#Path to your dataset files in Google Drive\n",
    "\n",
    "# Adjust these paths to match the actual location of your files\n",
    "drive_path = '/work/google-drive-hlt-files/'  # Base Google Drive path\n",
    "train_path = f\"{drive_path}/train_subset.csv\"\n",
    "val_path = f\"{drive_path}/val_subset.csv\"\n",
    "test_path = f\"{drive_path}/remaining_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9f3bde963795441ba75fcd1770f15ab7",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "eb5f18045e1348c7a6b00b4da51eb4ec",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 4375,
    "execution_start": 1746591538482,
    "source_hash": "80e7c50b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e68fb8bca214412e80b6806edda88582",
    "deepnote_cell_type": "separator"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "8e6f79891e6d47e39f7d21eb4ce590f2",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 33268,
    "execution_start": 1746591542912,
    "source_hash": "761ec655"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 20000 examples [00:00, 23315.49 examples/s]\n",
      "Generating validation split: 5000 examples [00:00, 16969.97 examples/s]\n",
      "Generating test split: 457235 examples [00:26, 17055.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_path,\n",
    "                                           'validation': val_path,\n",
    "                                           'test': test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5c9c8c931d5c4751a54c2fdfd8fcdc0a",
    "deepnote_cell_type": "separator"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9b802f0fc4794c45a198ac939063c12b",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 391867,
    "execution_start": 1746591576242,
    "source_hash": "18743a26"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Add WeightedTrainer class definition here:\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits = logits.float()\n",
    "        # Use CrossEntropyLoss with class weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Check class distribution\n",
    "train_labels = dataset[\"train\"][\"generated\"]\n",
    "class_distribution = Counter(train_labels)\n",
    "print(\"Original class distribution:\", class_distribution)\n",
    "\n",
    "# If there's imbalance, augment minority class\n",
    "total = sum(class_distribution.values())\n",
    "if any(class_distribution[k] / total < 0.4 for k in class_distribution):\n",
    "    minority_class = min(class_distribution, key=class_distribution.get)\n",
    "\n",
    "    def augment_text(examples):\n",
    "        aug = naw.SynonymAug(aug_min=1, aug_max=3)\n",
    "        augmented_texts = []\n",
    "        for text in examples['text']:\n",
    "            # Make sure to return a single string, not a list\n",
    "            if isinstance(text, str):\n",
    "                augmented = aug.augment(text)\n",
    "                # augment returns a list, so we take the first element\n",
    "                if isinstance(augmented, list):\n",
    "                    augmented = augmented[0]\n",
    "                augmented_texts.append(augmented)\n",
    "            else:\n",
    "                augmented_texts.append(text)\n",
    "        examples['text'] = augmented_texts\n",
    "        return examples\n",
    "\n",
    "    # Apply to minority class only\n",
    "    minority_dataset = dataset[\"train\"].filter(\n",
    "        lambda x: x[\"generated\"] == minority_class\n",
    "    ).map(augment_text, batched=True)\n",
    "\n",
    "    # Combine with original data\n",
    "    from datasets import concatenate_datasets\n",
    "    balanced_dataset = concatenate_datasets([dataset[\"train\"], minority_dataset])\n",
    "    dataset[\"train\"] = balanced_dataset\n",
    "\n",
    "    # Verify new distribution\n",
    "    new_distribution = Counter(dataset[\"train\"][\"generated\"])\n",
    "    print(\"Balanced class distribution:\", new_distribution)\n",
    "\n",
    "# Proceed with tokenization and class weights computation\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original text column, set format for PyTorch/TensorFlow\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"generated\", \"labels\") # Trainer expects 'labels'\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_labels = tokenized_datasets[\"train\"][\"labels\"]\n",
    "\n",
    "# Convert to numpy if it's a tensor\n",
    "if torch.is_tensor(train_labels):\n",
    "    train_labels = train_labels.cpu().numpy()\n",
    "else:\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e4e4c23e412a44e995b70eb2cbac1a90",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Fine Tuning portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c5b0de3d8709436d8deded1beeec03a3",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Load Pre-trained Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d26239fe81ed44288e2658186160e8d2",
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 1600,
    "execution_start": 1746591968162,
    "is_output_hidden": true,
    "source_hash": "b494947e"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to huggingface\n",
    "login(token=\"\",new_session = True, write_permission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ed17c3cdd0c249fdae29577688e6f34d",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "ebaa4ae82d3f42a5809044ec1fae9a47",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 0,
    "execution_start": 1746591969812,
    "source_hash": "c8e5a9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "print(\"Accelerate loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f546ac7a820247c4a4d8b6b9c05ea234",
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 0,
    "execution_start": 1746591969862,
    "is_output_hidden": true,
    "source_hash": "2cbbda31"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    username = user_info[\"name\"]\n",
    "    print(f\"Logged in to Hugging Face as: {username}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting Hugging Face user info: {e}\")\n",
    "    print(\"Make sure you're properly logged in with notebook_login()\")\n",
    "    username = None\n",
    "\n",
    "if username:\n",
    "    hub_model_id = f\"{username}/hlt-bert-text-classification\"  # Use your actual username\n",
    "else:\n",
    "    hub_model_id = None\n",
    "    print(\"Will not push to Hub due to authentication issues\")\n",
    "\n",
    "from transformers import EarlyStoppingCallback, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\", # Make sure eval_strategy is \"steps\" or \"epoch\"\n",
    "    eval_steps=100,       # How often to evaluate and check for early stopping\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,    # Important for early stopping\n",
    "    metric_for_best_model=\"f1\",     # Metric to monitor\n",
    "    greater_is_better=True,         # Whether a higher value of the metric is better\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "\n",
    "    # Add Hub parameters only if have a valid username\n",
    "    push_to_hub=username is not None,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"checkpoint\" if username else None,\n",
    "\n",
    "    # early_stopping_patience=3,\n",
    "    # early_stopping_threshold=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "253997486f7b4619ade45930e00eecb0",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00f6e77387e74b969872047737bf1ee3",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 165,
    "execution_start": 1746591969912,
    "source_hash": "b8b9cc14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 10.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\") # Load standard accuracy metric (or f1, precision, recall)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Load multiple metrics\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    precision = evaluate.load(\"precision\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6d0ff041277f41dc9324a9eca89da24a",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ec10899253d4452badcaf5d9a681b314",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 14106,
    "execution_start": 1746591970132,
    "source_hash": "899413cc"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Cnvert labels to long\n",
    "def convert_labels_to_long(examples):\n",
    "    examples[\"labels\"] = [int(label) for label in examples[\"labels\"]]\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels_to_long, batched=True)\n",
    "\n",
    "\n",
    "# Ensure format\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Init early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.005)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "65c7454a5111445ea6bdbbbe0af1f261",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "10eb3f45295944bb9cf5d9d57afcef99",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 0,
    "execution_start": 1746591984302,
    "source_hash": "6abee733"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Calculate total steps\n",
    "num_train_examples = len(tokenized_datasets[\"train\"])\n",
    "per_device_batch_size = training_args.per_device_train_batch_size\n",
    "gradient_accumulation_steps = training_args.gradient_accumulation_steps if hasattr(training_args, 'gradient_accumulation_steps') else 1\n",
    "num_gpus = 1\n",
    "\n",
    "total_batch_size = per_device_batch_size * gradient_accumulation_steps * num_gpus\n",
    "total_steps = (num_train_examples // total_batch_size) * training_args.num_train_epochs\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=training_args.learning_rate,\n",
    "    weight_decay=training_args.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Add optimizer and scheduler to trainer BEFORE training\n",
    "trainer.optimizer = optimizer\n",
    "trainer.lr_scheduler = scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b244ed7903634fe0881cd0b7b718a73f",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "ca9fd0ada44a4428843bee646a596cf2",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 1167794,
    "execution_start": 1746591984712,
    "source_hash": "ff0b543d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4290 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.79k/6.79k [00:00<00:00, 11.5MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.56k/7.56k [00:00<00:00, 16.4MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.38k/7.38k [00:00<00:00, 14.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.14013625741004943, metrics={'train_runtime': 1119.9737, 'train_samples_per_second': 122.525, 'train_steps_per_second': 3.83, 'total_flos': 6735643017216000.0, 'train_loss': 0.14013625741004943, 'epoch': 0.9324009324009324})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "21f10220c3a249e8b3d472beabdcb054",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Push Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "34c31b14b76642dfbec6198127f3c503",
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 15010,
    "execution_start": 1746593152552,
    "is_output_hidden": true,
    "source_hash": "e88dc001"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Push the model to Hugging Face Hub if it hasn't been automatically pushed during training\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed successfully to: https://huggingface.co/{training_args.hub_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4268fec79e7b434c81224fb7f5649c20",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "624f7122e6f445109447276b74c042ff",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 3324090,
    "execution_start": 1746593167612,
    "source_hash": "619c35f6"
   },
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(test_results)\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame([test_results])\n",
    "\n",
    "# Save to CSV locally first\n",
    "csv_path = 'test_results_final.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Download locally\n",
    "files.download(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bda9374108bc444289288383a0961f3d",
    "deepnote_cell_type": "code",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 144,
    "execution_start": 1746596617802,
    "source_hash": "6cbf899e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame([test_results])\n",
    "\n",
    "# Save to CSV locally first\n",
    "csv_path = 'test_results_final.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "110655f8902a4093a4cddb472a58e671",
    "color": "purple",
    "deepnote_cell_type": "text-cell-callout"
   },
   "source": [
    "> Download csv_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "acd54d22ad4545678dada319d8e67d29",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "49b62dc4-980d-4f67-a5b9-01c89344cbdf",
    "execution_millis": 1,
    "execution_start": 1746596679521,
    "source_hash": "f92e4d6",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.move('test_results_final.csv', '/work/test_results_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "92759e062ed5491896e10c027617eba4",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "The file `test_results_final.csv` has been moved and is now available at `/work/test_results_final.csv`. You can download it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=107e1980-0943-4584-b9d8-50a47211e48c' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "cfa2ca1b2a9b4e42b0bbc5f25e1c0d5a",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
